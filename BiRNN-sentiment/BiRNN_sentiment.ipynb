{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9oWnEwT1Bdyk"
   },
   "source": [
    "# Sentiment analysis using BiRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3FWAWHMUGMsi",
    "outputId": "047bced0-5528-41ea-a056-11a0440a4b0b"
   },
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TvRB0Gp6BcfO"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 217,
     "referenced_widgets": []
    },
    "id": "BBYkTUXlCp2H",
    "outputId": "cd3a42e7-104f-4fed-adf4-85fa6ecd38a0"
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"behbudiy/uzbek-sentiment-analysis\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "e9daa9f77ac64b7198f97f25bffd51f7",
      "a22513e9bf634aeaaf9fa16650051cf1",
      "02e841810d3c4cdba033b15ab13e97f6",
      "c49c9cac719d47b181ba2517d49efd04",
      "6d8758ee75e04797a8efd647c45f67ac",
      "2841784c0a554b948ea402db12b4521f",
      "9977e91b21fa4ca798c20eb1b95ba5de",
      "25a566b7b89248c998ce82bf6134d055",
      "31661b06790b48fcaa28e994c3f19d5a",
      "130746905a9145cf955da098a88e3b45",
      "655fa235943844c7854d676ef8954ae8",
      "208795b12a1f472686f5b36ad6d1450c",
      "d43d38fcfeb74190a6e1c1fd09b18a47",
      "0b10e4bc668a4fcfa74dacf3e314db8b",
      "0722fb4fdfdd4886950a9093b69b6681",
      "f1d65f28120e41c0ba75ff06072c852f",
      "7e5decb5dc2841e6b86ae12e07666013",
      "89a4effb4e014e07a704201b0af55c4b",
      "83a90605744c41ae951aba851715b9fe",
      "ffeb767aa69349f5a51cf290ff74b829",
      "46c82952cd3a47b69fe80b08c4d727ba",
      "87061fb6335e43faa465f7fa8756033e",
      "c7bf8a070a8e44348fbd3610ea049eed",
      "30b69257ca894509a37c3364d31ff6a7",
      "b5d55968e4244d559d6402d57d9bcd52",
      "b3f4cb3be9bd479cb285b6afcf4fc3e1",
      "c7f5253ba37640e5be81dfc7b7821948",
      "a84fa120829c490db1cf5f2d11d029ba",
      "e3c028dac1d34473b3d1f549dd721b1e",
      "ce6000e67fcf4237b1e5710b5de2ed0e",
      "6622f43309d4470ca025efacf144de2d",
      "4d5dd258d17d405fac5f305751929e97",
      "42b163a773ab46e69ac764caa780d96d",
      "b3cff1cce6254c1c964d933fc929ac06",
      "c38b679eb86047f9b2ac2c38d7bd409b",
      "5d3e66314a004cd88d6992cf5784f112",
      "a9090574731e44ab882aae254298f0e9",
      "b0fe9ce7ea814a95ab7c7e7d40c8b76f",
      "a695ec49f51841f1a4230b8b9b24374e",
      "22593f9578344c238ce893c1820d81c1",
      "b5cd3c3e4db341a39d661c3a32ee334a",
      "879e86dba2434a8fb0c175c620920381",
      "b63abc56ccef4460a860cba90f5fa422",
      "8ace33f93ae248f098f5227998f9f6d5"
     ]
    },
    "id": "uvK3Fi6YCisS",
    "outputId": "06cc2c2d-b677-41cb-9ed3-1298228ddafc"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JUTPpNv3DMnR"
   },
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "d6fc97c900cb4a0693bd85373b533d0f",
      "c67267da568d4551b49d6af1e470456e",
      "e6b48210735c40a4803020c89749cbf1",
      "995d194e171a4527816aef2b65be4855",
      "feba41afac4e4c61aedff2096f9f8ac9",
      "2a6450d9d6504ac591f60b4443c657c3",
      "4b40dc4e904f40f6825e53fd5620e194",
      "85b0be81dfbf4f4d95b9054d21fc936c",
      "90590980874a41908de066e3b10a0528",
      "9ec57618ee674f49b381a11edf27a996",
      "3733606860484e43a35ac5fb3c5731dd"
     ]
    },
    "id": "p80I6wegFCbI",
    "outputId": "cd4c34a1-4753-4e65-e91d-e81fb557e626"
   },
   "outputs": [],
   "source": [
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nj1Rgeb-FEdJ"
   },
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(tokenized_datasets[\"train\"]))  # 80% for training\n",
    "test_size = len(tokenized_datasets[\"train\"]) - train_size  # 20% for testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mgop3SRYFJHU"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "train_dataset, test_dataset = random_split(tokenized_datasets[\"train\"], [train_size, test_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KjncuYYzFr9x"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2iFlKZg2FvFm"
   },
   "outputs": [],
   "source": [
    "class BiRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers):\n",
    "        super(BiRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.encoder = nn.LSTM(embed_size, num_hiddens, num_layers=num_layers, bidirectional=True)\n",
    "        self.decoder = nn.Linear(4 * num_hiddens, 2)  # Binary classification\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeddings = self.embedding(inputs.T)  # Transpose for LSTM\n",
    "        self.encoder.flatten_parameters()\n",
    "        outputs, _ = self.encoder(embeddings)  # Outputs shape: (time steps, batch size, 2*num_hiddens)\n",
    "        encoding = torch.cat((outputs[0], outputs[-1]), dim=1)  # Concatenate first and last states\n",
    "        outs = self.decoder(encoding)\n",
    "        return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EoWEhjehFyDx"
   },
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.vocab_size\n",
    "embed_size, num_hiddens, num_layers = 100, 100, 2\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RjdSHC1HGeIU"
   },
   "outputs": [],
   "source": [
    "model = BiRNN(vocab_size, embed_size, num_hiddens, num_layers).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9UoiquUNGhqG"
   },
   "outputs": [],
   "source": [
    "# def train_model(model, train_loader, test_loader, num_epochs, criterion, optimizer):\n",
    "#     model.train()\n",
    "#     for epoch in range(num_epochs):\n",
    "#         total_loss = 0\n",
    "#         for batch in train_loader:\n",
    "#             inputs = batch[\"input_ids\"].to(device)\n",
    "#             labels = batch[\"labels\"].to(device)\n",
    "\n",
    "\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs, labels)\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             total_loss += loss.item()\n",
    "\n",
    "#         print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "#     evaluate_model(model, test_loader)\n",
    "\n",
    "\n",
    "# def evaluate_model(model, test_loader):\n",
    "#     model.eval()\n",
    "#     correct, total = 0, 0\n",
    "#     with torch.no_grad():\n",
    "#         for batch in test_loader:\n",
    "#             inputs = batch[\"input_ids\"].to(device)\n",
    "#             labels = batch[\"labels\"].to(device)\n",
    "#             outputs = model(inputs)\n",
    "#             predictions = torch.argmax(outputs, dim=1)\n",
    "#             correct += (predictions == labels).sum().item()\n",
    "#             total += labels.size(0)\n",
    "#     print(f\"Accuracy: {correct / total:.4f}\")\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "def train_model(model, train_loader, test_loader, num_epochs, criterion, optimizer):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            inputs = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    evaluate_model(model, test_loader)\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            inputs = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            predictions = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            all_preds.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = (np.array(all_preds) == np.array(all_labels)).mean()\n",
    "    precision = precision_score(all_labels, all_preds, zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, zero_division=0)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HoYxLxLlGzoS",
    "outputId": "b68f8eaf-bc8a-4027-9b99-df10c9ac699e"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "num_epochs = 5\n",
    "train_model(model, train_dataloader, test_dataloader, num_epochs, criterion, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "edtPop9PG8KS"
   },
   "outputs": [],
   "source": [
    "def predict_sentiment(model, tokenizer, text):\n",
    "    model.eval()\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=128)\n",
    "    inputs = tokens[\"input_ids\"].to(device)  # No need to transpose here\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs)  # Directly pass inputs\n",
    "        label = torch.argmax(outputs, dim=1).item()  # Take argmax over class dimension\n",
    "    return \"positive\" if label == 1 else \"negative\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c14Jo0jEG-L5",
    "outputId": "f216d192-2610-4129-ae17-aae477a2d874"
   },
   "outputs": [],
   "source": [
    "\n",
    "print(predict_sentiment(model, tokenizer, \"Oka yaxwi chqmapt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "filename = \"BiRNN_sentiment.ipynb\"  \n",
    "\n",
    "with open(filename, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "if 'widgets' in data.get('metadata', {}):\n",
    "    print(\"Fixing metadata.widgets...\")\n",
    "    del data['metadata']['widgets']\n",
    "\n",
    "with open(filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(data, f, indent=2)\n",
    "\n",
    "print(\"Notebook metadata fixed!\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "experiments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
